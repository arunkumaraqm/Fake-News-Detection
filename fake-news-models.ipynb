{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5254c8068ec6a28f28d7240547fe46da68ed71b29d175470102f3b8f29d4e88e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aqm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aqm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') # for nltk's sentence tokenization within word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Index(['title', 'text', 'subject', 'date', 'label'], dtype='object'),\n",
       " 21417,\n",
       " 23481)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "path = 'kaggle-fn-dataset'\n",
    "path_real = os.path.join(path, 'True.csv')\n",
    "path_fake = os.path.join(path, 'Fake.csv')\n",
    "\n",
    "realdf = pd.read_csv(path_real)\n",
    "realdf['label'] = True\n",
    "fakedf = pd.read_csv(path_fake)\n",
    "fakedf['label'] = False\n",
    "realdf.columns, len(realdf), len(fakedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(44898, 33673, 11225)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df = pd.concat([realdf, fakedf])\n",
    "traindf, testdf = train_test_split(df)\n",
    "len(df), len(traindf), len(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = traindf.label\n",
    "ytest = testdf.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(model, xtrain, ytrain, xtest, ytest):\n",
    "    model.fit(xtrain, ytrain)\n",
    "    # train_score = model.score(xtrain, ytrain)\n",
    "    test_score = model.score(xtest, ytest)\n",
    "    print('For', model)\n",
    "    # print('Train score: ', train_score)    \n",
    "    print('Test score: ', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('countvectorizer', CountVectorizer(max_features=50)),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.878300121759273\nTest score:  0.8769710467706013\n"
     ]
    }
   ],
   "source": [
    "model_bow_nb = make_pipeline(CountVectorizer(max_features=50), MultinomialNB())\n",
    "train_and_score(model_bow_nb, traindf.text, ytrain, testdf.text, ytest) # 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('countvectorizer',\n                 CountVectorizer(max_features=50, ngram_range=(2, 2))),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.8404062602084756\nTest score:  0.8457015590200445\n"
     ]
    }
   ],
   "source": [
    "model_bigram_nb = make_pipeline(CountVectorizer(max_features=50, ngram_range=(2, 2)), MultinomialNB())\n",
    "train_and_score(model_bigram_nb, traindf.text, ytrain, testdf.text, ytest) # 84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_features=50)),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.8659757075401657\nTest score:  0.8679732739420936\n"
     ]
    }
   ],
   "source": [
    "model_tfidf_nb = make_pipeline(TfidfVectorizer(max_features=50), MultinomialNB())\n",
    "train_and_score(model_tfidf_nb, traindf.text, ytrain, testdf.text, ytest) # 87%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('countvectorizer',\n                 CountVectorizer(max_features=50,\n                                 stop_words={'a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...})),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.9129866658747364\nTest score:  0.9168819599109131\n"
     ]
    }
   ],
   "source": [
    "cv_with_stop_words = CountVectorizer(\n",
    "    max_features=50, \n",
    "    stop_words=custom_stop_words\n",
    ")\n",
    "model_bow_sw_nb = make_pipeline(cv_with_stop_words, MultinomialNB())\n",
    "\n",
    "train_and_score(model_bow_sw_nb, traindf.text, ytrain, testdf.text, ytest) # 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('countvectorizer',\n                 CountVectorizer(max_features=50, ngram_range=(2, 2),\n                                 stop_words={'a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...})),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.8733406586879696\nTest score:  0.864053452115813\n"
     ]
    }
   ],
   "source": [
    "cv_bigram_with_stop_words = CountVectorizer(\n",
    "    max_features=50, \n",
    "    ngram_range=(2, 2),\n",
    "    stop_words=custom_stop_words\n",
    ")\n",
    "model_bigram_sw_nb = make_pipeline(cv_bigram_with_stop_words, MultinomialNB())\n",
    "\n",
    "train_and_score(model_bigram_sw_nb, traindf.text, ytrain, testdf.text, ytest) # 86%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[(\"nltk's tokenizer\",\n                 CountVectorizer(max_features=50,\n                                 stop_words={'a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...},\n                                 tokenizer=<function word_tokenize at 0x7fd612dd8670>)),\n                ('mnb', MultinomialNB())])\nTest score:  0.9889532293986637\nCPU times: user 3min 2s, sys: 463 ms, total: 3min 2s\nWall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(\n",
    "    max_features=50, \n",
    "    stop_words=custom_stop_words,\n",
    "    tokenizer=nltk.word_tokenize\n",
    ")\n",
    "temp = Pipeline(steps=[\n",
    "    ('nltk\\'s tokenizer', cv), \n",
    "    ('mnb', MultinomialNB()),\n",
    "])\n",
    "train_and_score(temp, traindf.text, ytrain, testdf.text, ytest) # 0.9889; 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[(\"nltk's tokenizer\",\n                 TfidfVectorizer(max_features=50,\n                                 stop_words={'a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...},\n                                 tokenizer=<function word_tokenize at 0x7fd612dd8670>)),\n                ('mnb', MultinomialNB())])\nTest score:  0.9890423162583519\nCPU times: user 2min 38s, sys: 508 ms, total: 2min 38s\nWall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = TfidfVectorizer(\n",
    "    max_features=50, \n",
    "    stop_words=custom_stop_words,\n",
    "    tokenizer=nltk.word_tokenize\n",
    ")\n",
    "temp = Pipeline(steps=[\n",
    "    ('nltk\\'s tokenizer', cv), \n",
    "    ('mnb', MultinomialNB()),\n",
    "])\n",
    "train_and_score(temp, traindf.text, ytrain, testdf.text, ytest) # 0.9890; 3 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerAndStemmer(object):\n",
    "\n",
    "    stem = nltk.PorterStemmer().stem\n",
    "    def __call__(self, text):\n",
    "        return (self.stem(itoken) for itoken in nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[(\"porter stemmer + nltk's tokenizer\",\n                 CountVectorizer(max_features=50,\n                                 stop_words={'a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...},\n                                 tokenizer=<__main__.TokenizerAndStemmer object at 0x7fd60e21ab20>)),\n                ('mnb', MultinomialNB())])\nTest score:  0.9897550111358575\nCPU times: user 6min 47s, sys: 188 ms, total: 6min 47s\nWall time: 6min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(\n",
    "    max_features=50, \n",
    "    stop_words=custom_stop_words,\n",
    "    tokenizer=TokenizerAndStemmer()\n",
    ")\n",
    "temp = Pipeline(steps=[\n",
    "    ('porter stemmer + nltk\\'s tokenizer', cv), \n",
    "    ('mnb', MultinomialNB()),\n",
    "])\n",
    "train_and_score(temp, traindf.text, ytrain, testdf.text, ytest) # 0.9897; 7 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
