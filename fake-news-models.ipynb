{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('minconda3': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5254c8068ec6a28f28d7240547fe46da68ed71b29d175470102f3b8f29d4e88e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'kaggle-fn-dataset'\n",
    "path_real = os.path.join(path, 'True.csv')\n",
    "path_fake = os.path.join(path, 'Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Index(['title', 'text', 'subject', 'date'], dtype='object'), 21417)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "realdf = pd.read_csv(path_real)\n",
    "realdf['label'] = True\n",
    "realdf.columns, len(realdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Index(['title', 'text', 'subject', 'date', 'label'], dtype='object'), 23481)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "fakedf = pd.read_csv(path_fake)\n",
    "fakedf['label'] = False\n",
    "fakedf.columns, len(fakedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Index(['title', 'text', 'subject', 'date', 'label'], dtype='object'), 44898)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df = pd.concat([realdf, fakedf])\n",
    "traindf, testdf = train_test_split(df)\n",
    "len(df), len(traindf), len(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = traindf.label\n",
    "ytest = testdf.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score(model, xtrain, ytrain, xtest, ytest):\n",
    "    model.fit(xtrain, ytrain)\n",
    "    train_score = model.score(xtrain, ytrain)\n",
    "    test_score = model.score(xtest, ytest)\n",
    "    print('For', model)\n",
    "    print('Train score: ', train_score)    \n",
    "    print('Test score: ', test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For MultinomialNB()\nTrain score:  0.878300121759273\nTest score:  0.8769710467706013\n"
     ]
    }
   ],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "train_and_score(mnb_model, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('countvectorizer', CountVectorizer(max_features=50)),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.878300121759273\nTest score:  0.8769710467706013\n"
     ]
    }
   ],
   "source": [
    "model_bow_nb = make_pipeline(CountVectorizer(max_features=50), MultinomialNB())\n",
    "train_and_score(model_bow_nb, traindf.text, ytrain, testdf.text, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('countvectorizer',\n                 CountVectorizer(max_features=50, ngram_range=(2, 2))),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.8404062602084756\nTest score:  0.8457015590200445\n"
     ]
    }
   ],
   "source": [
    "model_bigram_nb = make_pipeline(CountVectorizer(max_features=50, ngram_range=(2, 2)), MultinomialNB())\n",
    "train_and_score(model_bigram_nb, traindf.text, ytrain, testdf.text, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_features=50)),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.8659757075401657\nTest score:  0.8679732739420936\n"
     ]
    }
   ],
   "source": [
    "model_tfidf_nb = make_pipeline(TfidfVectorizer(max_features=50), MultinomialNB())\n",
    "train_and_score(model_tfidf_nb, traindf.text, ytrain, testdf.text, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('countvectorizer',\n                 CountVectorizer(max_features=50,\n                                 stop_words={'a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...})),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.9129866658747364\nTest score:  0.9168819599109131\n"
     ]
    }
   ],
   "source": [
    "cv_with_stop_words = CountVectorizer(\n",
    "    max_features=50, \n",
    "    stop_words=custom_stop_words\n",
    ")\n",
    "model_bow_sw_nb = make_pipeline(cv_with_stop_words, MultinomialNB())\n",
    "\n",
    "train_and_score(model_bow_sw_nb, traindf.text, ytrain, testdf.text, ytest) # 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Pipeline(steps=[('countvectorizer',\n                 CountVectorizer(max_features=50, ngram_range=(2, 2),\n                                 stop_words={'a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...})),\n                ('multinomialnb', MultinomialNB())])\nTrain score:  0.8733406586879696\nTest score:  0.864053452115813\n"
     ]
    }
   ],
   "source": [
    "cv_bigram_with_stop_words = CountVectorizer(\n",
    "    max_features=50, \n",
    "    ngram_range=(2, 2),\n",
    "    stop_words=custom_stop_words\n",
    ")\n",
    "model_bigram_sw_nb = make_pipeline(cv_bigram_with_stop_words, MultinomialNB())\n",
    "\n",
    "train_and_score(model_bigram_sw_nb, traindf.text, ytrain, testdf.text, ytest) # 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
